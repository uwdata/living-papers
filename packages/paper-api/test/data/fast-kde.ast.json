{"metadata":{"author":[{"city":"Seattle","country":"United States","name":"Jeffrey Heer","orcid":"0000-0002-6175-1655","org":"University of Washington","state":"WA"}],"doi":"10.1109/VIS49827.2021.9623323","keywords":["Kernel density estimation","Gaussian convolution","binning","approximation","performance","evaluation"],"output":{"ast":true,"esm":true,"html":true,"latex":{"template":"ieee-vgtc-conference","vspace":{"caption":"-10pt","equation":"-15pt","figure":"-15pt","teaser":"-15pt"}}},"title":"Fast & Accurate Gaussian Kernel Density Estimation","venue":"IEEE Visualization","year":"2021"},"article":{"type":"component","name":"article","children":[{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"kde_impulse"},"class":{"type":"value","value":"teaser"}},"children":[{"type":"component","name":"image","properties":{"src":{"type":"value","value":"figures/impulse_1d.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"Gaussian kernel density estimation for a single impulse value ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins, "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" = 0.2). Iterated uniform ("},{"type":"component","name":"quote","children":[{"type":"textnode","value":"box"}]},{"type":"textnode","value":") filters "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Gwosdek:2011"},"index":{"type":"value","value":8}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wells:1986"},"index":{"type":"value","value":26}}}]},{"type":"textnode","value":" (red & dashed) underestimate the mode and overestimate the sides of the distribution. Deriche’s "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1990"},"index":{"type":"value","value":4}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1993"},"index":{"type":"value","value":5}}}]},{"type":"textnode","value":" linear-time recursive filter approximation (blue) produces a pixel-perfect match to the true distribution (grey)."}]}]},{"type":"component","name":"abstract","children":[{"type":"component","name":"p","children":[{"type":"textnode","value":"Kernel density estimation (KDE) models a discrete sample of data as a continuous distribution, supporting the construction of visualizations such as violin plots, heatmaps, and contour plots.\nThis paper draws on the statistics and image processing literature to survey efficient and scalable density estimation techniques for the common case of Gaussian kernel functions.\nWe evaluate the accuracy and running time of these methods across multiple visualization contexts and find that the combination of linear binning and a recursive filter approximation by Deriche efficiently produces pixel-perfect estimates across a compelling range of kernel bandwidths."}]}]},{"type":"component","name":"h1","properties":{"id":{"type":"value","value":"introduction"}},"children":[{"type":"textnode","value":"Introduction"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Kernel density estimation ("},{"type":"component","name":"em","children":[{"type":"textnode","value":"KDE"}]},{"type":"textnode","value":") "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Parzen:1962"},"index":{"type":"value","value":14}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Rosenblatt:1956"},"index":{"type":"value","value":17}}}]},{"type":"textnode","value":" estimates a continuous probability density function for a finite sample of data.\nKDE is regularly used to visualize univariate distributions for exploratory analysis in the form of area charts or violin plots "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Correll:2014"},"index":{"type":"value","value":3}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Hintzel:1998"},"index":{"type":"value","value":9}}}]},{"type":"textnode","value":", providing valuable alternatives to histograms.\nIn two dimensions, KDE estimates produce smoothed heatmaps that can be visualized directly as images or used to extract density isolines "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Lorensen:1987:MCA"},"index":{"type":"value","value":13}}}]},{"type":"textnode","value":" for contour plots."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"To form a density estimate, each data point is modeled as a probability distribution, or "},{"type":"component","name":"em","children":[{"type":"textnode","value":"kernel"}]},{"type":"textnode","value":", centered at that point.\nThe kernel is parameterized by a "},{"type":"component","name":"em","children":[{"type":"textnode","value":"bandwidth"}]},{"type":"textnode","value":" "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" that determines the width (or spread) of each point distribution.\nThe sum of these kernels constitutes the density estimate for the sample.\nWhile a variety of kernel functions exist, the normal (Gaussian) distribution is a common choice "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Scott:1992"},"index":{"type":"value","value":21}}}]},{"type":"textnode","value":", in which case the bandwidth "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" is its standard deviation."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We would like density estimation to be "},{"type":"component","name":"em","children":[{"type":"textnode","value":"fast"}]},{"type":"textnode","value":": scalable to large datasets, yet amenable to interactive bandwidth adjustment.\nA naïve calculation has quadratic "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(m * n)"}}},{"type":"textnode","value":" complexity: we must sum the contributions of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" data points at each of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" locations at which we measure the density.\nWhile approximation methods exist, we want them to be "},{"type":"component","name":"em","children":[{"type":"textnode","value":"accurate"}]},{"type":"textnode","value":", as inaccurate estimates can result in visualizations with missing features or false local extrema (peaks or valleys)."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"This paper reviews scalable, linear-time approximations of Gaussian kernel densities that smooth a binned grid of values.\nWe evaluate a set of methods – "},{"type":"component","name":"em","children":[{"type":"textnode","value":"box filters"}]},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wand:1994"},"index":{"type":"value","value":25}}}]},{"type":"textnode","value":", "},{"type":"component","name":"em","children":[{"type":"textnode","value":"extended box filters"}]},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Gwosdek:2011"},"index":{"type":"value","value":8}}}]},{"type":"textnode","value":", and "},{"type":"component","name":"em","children":[{"type":"textnode","value":"Deriche’s approximation"}]},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1990"},"index":{"type":"value","value":4}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1993"},"index":{"type":"value","value":5}}}]},{"type":"textnode","value":" – in the context of 1D area charts and 2D heatmaps.\nWe find that the combination of linear binning (proportionally dividing the weight of a point among adjacent bins) and Deriche’s approximation is both fast and highly accurate, outperforming methods currently used in existing visualization tools and often providing pixel-perfect results."}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\place{err1d_impulse}"}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\place{err1d_penguins}"}]},{"type":"component","name":"h1","properties":{"id":{"type":"value","value":"density-estimation-methods"}},"children":[{"type":"textnode","value":"Density Estimation Methods"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Given a dataset with "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" data points "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x_i \\in \\Re"}}},{"type":"textnode","value":", a kernel function "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"K"}}},{"type":"textnode","value":", and bandwidth "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":", the univariate kernel density estimate is defined as:"}]},{"type":"component","name":"equation","children":[{"type":"textnode","value":"f(x) = \\frac{1}{n\\sigma} \\sum_{i=1}^{n} K{\\Big (}\\frac{x - x_i}{\\sigma}{\\Big )}"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We focus on the case where "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"K"}}},{"type":"textnode","value":" is the normal (Gaussian) density "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"K(x) = \\frac{1}{\\sqrt {2\\pi}} e^{-{x^2} / 2}"}}},{"type":"textnode","value":".\nWe can directly calculate the density at a point "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x"}}},{"type":"textnode","value":" by summing the kernel response for each data point.\nIf we query the density at "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" measurement points, this approach has computational complexity "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(m * n)"}}},{"type":"textnode","value":", which for large datasets can be untenable."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Nevertheless, direct calculation is used by multiple tools.\nAt the time of writing, Vega and Vega-Lite "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Satyanarayan:2015"},"index":{"type":"value","value":18}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Satyanarayan:2016"},"index":{"type":"value","value":19}}}]},{"type":"textnode","value":" use direct calculation for one-dimensional KDE, where "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" is the number of points queried in order to draw the density.\nLine segments then connect these measured values.\nThe "},{"type":"component","name":"code","children":[{"type":"textnode","value":"kde2d"}]},{"type":"textnode","value":" function of R’s MASS "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"MASS:2002"},"index":{"type":"value","value":24}}}]},{"type":"textnode","value":" package (invoked by the popular ggplot2 "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wickham:2009"},"index":{"type":"value","value":27}}}]},{"type":"textnode","value":" library for 2D density estimates) also uses a direct calculation approach, limiting the feasible number of measurement points for plotting.\nOne can optimize direct calculation by leveraging spatial indices (e.g., KD trees or Ball trees) to approximate the contribution of suitably distant points "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Gray:2003"},"index":{"type":"value","value":7}}}]},{"type":"textnode","value":", as done in the Python scikit-learn library "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"ScikitLearn"},"index":{"type":"value","value":15}}}]},{"type":"textnode","value":".\nHowever, the asymptotic complexity remains a superlinear function of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":"."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"To speed estimation, statisticians proposed "},{"type":"component","name":"em","children":[{"type":"textnode","value":"binned KDE"}]},{"type":"textnode","value":" methods "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Scott:1985"},"index":{"type":"value","value":20}}}]},{"type":"textnode","value":" that first aggregate input data into a uniform grid with "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" bins.\nKDE then reduces to the signal processing task of smoothing the binned data.\nFor example, one can directly convolve the binned values with a discrete Gaussian kernel (or "},{"type":"component","name":"em","children":[{"type":"textnode","value":"filter"}]},{"type":"textnode","value":").\nThe resulting complexity "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n + m * w)"}}},{"type":"textnode","value":" is dependent not just on the number of bins "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":", but also the filter width "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"w"}}},{"type":"textnode","value":".\nLarger bandwidths can result in filter widths on the same order as "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":", for a quadratic running time."}]},{"type":"component","name":"p","children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Silverman:1982"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":23}}},{"type":"textnode","value":" instead applies the Fast Fourier Transform (FFT), an approach used by R’s "},{"type":"component","name":"code","children":[{"type":"textnode","value":"density"}]},{"type":"textnode","value":" routine.\nA strength of this method is that it can support arbitrary kernel functions: the binned data and a discretized kernel response are separately mapped to the frequency domain using the FFT, the results are multiplied element-wise (convolution in the frequency domain), and an inverse FFT then produces the density estimate.\nThe complexity is "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n + m \\log m)"}}},{"type":"textnode","value":", with binning of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" points followed by FFT calls on "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":"-sized grids."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"For even faster estimates, linear-time "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n + m)"}}},{"type":"textnode","value":" approximations exist.\nThese are particularly attractive for 2D density estimation, which can be computed using a series of 1D convolutions along every row and every column of a binned 2D grid.\nOne can approximate 1D Gaussian convolution by iteratively applying a filter of uniform weight, also known as a "},{"type":"component","name":"strong","children":[{"type":"textnode","value":"box filter"}]},{"type":"textnode","value":".\n"},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wells:1986"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":26}}},{"type":"textnode","value":" applies this method to density estimation, contributing a formula for the filter length "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"w"}}},{"type":"textnode","value":" (or equivalently, its radius "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"r"}}},{"type":"textnode","value":") as a function of both "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" and the number of filter iterations "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"k"}}},{"type":"textnode","value":".\nAn attractive property of this approach is its simplicity of calculation: "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"k"}}},{"type":"textnode","value":" iterations of uniform filtering (running sums), followed by a scale adjustment.\nBoth d3-contour "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Bostock:2011"},"index":{"type":"value","value":1}}}]},{"type":"textnode","value":" and Vega use per-row and per-column box filters for 2D density estimation."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Box filtering runs in linear-time, but has important nuances.\nFirst, the bandwidth "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" (a continuous value) is discretized to a filter with integer radius "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"r"}}},{"type":"textnode","value":", leading to quantization error.\nTo address this issue, "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Gwosdek:2011"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":8}}},{"type":"textnode","value":" propose an "},{"type":"component","name":"strong","children":[{"type":"textnode","value":"extended box filter"}]},{"type":"textnode","value":" that introduces some non-uniformity by adding fractional weight to the endpoints of the filter.\nSecond, the true grid size is no longer a constant parameter such as "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins.\nAs iterated filters "},{"type":"component","name":"quote","properties":{"class":{"type":"value","value":"single"}},"children":[{"type":"textnode","value":"blur"}]},{"type":"textnode","value":" weight into adjacent bins, the grid must be extended on both ends by an offset of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"k * r"}}},{"type":"textnode","value":".\nThe running time scales as "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n + l"}}},{"type":"textnode","value":", where "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"l = m + 2k * r"}}},{"type":"textnode","value":".\nAs the filter radius "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"r"}}},{"type":"textnode","value":" is a monotone function of "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wells:1986"},"index":{"type":"value","value":26}}}]},{"type":"textnode","value":", this can result in larger grids – and thus higher time and memory costs – for larger bandwidths."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Finally, we consider an approximation developed by "},{"type":"component","name":"strong","children":[{"type":"textnode","value":"Deriche"}]},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1990"},"index":{"type":"value","value":4}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Deriche:1993"},"index":{"type":"value","value":5}}}]},{"type":"textnode","value":" for computer vision applications.\nDeriche models the right half of the standard Gaussian using an order-"},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"K"}}},{"type":"textnode","value":" approximation:"}]},{"type":"component","name":"equation","children":[{"type":"textnode","value":"h_{K}(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\n\\sum_{k=1}^{K} \\alpha_k e^{-x \\lambda_k / \\sigma}"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"From this formulation, Deriche constructs a recursive filter that proceeds linearly from the first value to the last. The left half of the Gaussian is defined by reversing the equation and subtracting the sample at "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x = 0"}}},{"type":"textnode","value":" (so that it is not included twice) to form a second filter. We use a fourth-order approximation ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"K = 4"}}},{"type":"textnode","value":"), with coefficients"}]},{"type":"component","name":"equation","properties":{"nonumber":{"type":"value","value":"true"}},"children":[{"type":"textnode","value":"\\alpha_1 & = 0.84 + i 1.8675, \\; & \\alpha_3 & = -0.34015 - i 0.1299 \\\\\n\\lambda_1 & = 1.783 + i 0.6318, \\; & \\lambda_3  &= 1.723 + i 1.997"}]},{"type":"component","name":"p","children":[{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\noindent "}]},{"type":"textnode","value":"and "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\alpha_{2k} = \\alpha_{2k-1}^{*}"}}},{"type":"textnode","value":", "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\lambda_{2k} = \\lambda_{2k-1}^{*}"}}},{"type":"textnode","value":", where "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x^*"}}},{"type":"textnode","value":" denotes the complex conjugate.\nDeriche determined the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\alpha_k"}}},{"type":"textnode","value":" and "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\lambda_k"}}},{"type":"textnode","value":" parameters using numerical optimization to find the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\ell^2"}}},{"type":"textnode","value":"-best fit to the Gaussian over the domain "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n = 0, \\dots, 1000"}}},{"type":"textnode","value":" with "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma = 100"}}},{"type":"textnode","value":".\n"},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Getreuer:2013"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":6}}},{"type":"textnode","value":" describes how to algebraically rearrange the terms of these filters into direct summations."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"After a constant time initialization to compute summation terms for a chosen "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":", the algorithm requires a linear pass over the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" bins for each filter, plus a final pass to sum their results.\nTo handle boundary values, the algorithm must in general perform iterative initialization per filter, requiring at most another linear pass.\nFortunately, this initialization reduces to a constant time operation for zero-padded data (i.e., where no weight resides outside the bins)."},{"type":"component","name":"note","children":[{"type":"textnode","value":"In contrast to zero-padded data, one must perform iterations for reflected signals (used in image processing to blur without decreasing image brightness) or periodic domains (where the final bin wraps back to the first)."}]},{"type":"textnode","value":"\nDeriche’s method has complexity "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n + m)"}}},{"type":"textnode","value":"; it involves more arithmetic operations per step than box filters, but does not require padding the binned grid.\nAs we will see, it is also much more accurate.\nTo the best of our knowledge, this work is the first to apply Deriche’s approximation to the task of kernel density estimation."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"While other methods for approximating Gaussian convolution have been proposed, they exhibit higher error rates and/or longer running times than those above.\nFor more details, see Getreuer’s survey and evaluation in the context of image filtering "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Getreuer:2013"},"index":{"type":"value","value":6}}}]},{"type":"textnode","value":"."}]},{"type":"component","name":"h1","properties":{"id":{"type":"value","value":"binning-schemes"}},"children":[{"type":"textnode","value":"Binning Schemes"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"For binned KDE approaches one must choose how to bin input data into a uniform grid.\nBy default we assume the weight of a data point is 1; however, a binned grid can easily accommodate variably-weighted points.\n"},{"type":"component","name":"strong","children":[{"type":"textnode","value":"Simple binning"}]},{"type":"textnode","value":", commonly performed for histograms, places all the weight for a data point into the single bin interval that contains the point.\n"},{"type":"component","name":"strong","children":[{"type":"textnode","value":"Linear binning"}]},{"type":"textnode","value":" "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Jones:1983"},"index":{"type":"value","value":11}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Wand:1994"},"index":{"type":"value","value":25}}}]},{"type":"textnode","value":" is an alternative that proportionally distributes the weight of a point between adjacent bins.\nIf a data point "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x_i"}}},{"type":"textnode","value":" lies between bins with midpoints "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"b_0"}}},{"type":"textnode","value":" and "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"b_1"}}},{"type":"textnode","value":", linear binning assigns weight proportional to "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"(b_1 - x_i) / (b_1 - b_0)"}}},{"type":"textnode","value":" to bin "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"b_0"}}},{"type":"textnode","value":" and "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"(x_i - b_0) / (b_1 - b_0)"}}},{"type":"textnode","value":" to bin "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"b_1"}}},{"type":"textnode","value":".\nFor example when a point lies at the exact center of a bin, that bin receives all the weight.\nIf a point resides near the boundary of two bins, its weight is split nearly evenly between them.\nWe examine both binning approaches in our evaluation below."}]},{"type":"component","name":"h1","properties":{"id":{"type":"value","value":"evaluation"}},"children":[{"type":"textnode","value":"Evaluation"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"How well do the above estimation methods perform in practice?\n"},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Getreuer:2013"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":6}}},{"type":"textnode","value":" evaluates a suite of Gaussian filtering methods in the context of image processing (e.g., blurring), inspiring the choice of methods we evaluate here.\nThat survey does not address the question of binning method (images are already discretized into pixels) and assumes reflected signals outside the image boundaries (for filters that preserve overall image brightness).\n"},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Bullman:2018"},"mode":{"type":"value","value":"inline-author"},"index":{"type":"value","value":2}}},{"type":"textnode","value":" examine approximate KDE methods for sensor fusion applications.\nThey do not evaluate visualization directly, and only assess box filter approaches, omitting alternative methods such as Deriche approximation."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We evaluate KDE methods in a visualization context, assessing box filters, extended box filters, and Deriche approximation.\nFor the box filter methods, we use "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"k"}}},{"type":"textnode","value":" = 3 iterations of filtering.\nUsing 4 or 5 iterations trades-off longer running times for higher accuracy, but the results remain qualitatively similar.\nFor the Deriche method, we use a 4th-order recursive filter approximation "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Getreuer:2013"},"index":{"type":"value","value":6}}}]},{"type":"textnode","value":". Datasets and benchmark scripts are included as supplemental material."}]},{"type":"component","name":"h2","properties":{"id":{"type":"value","value":"method"}},"children":[{"type":"textnode","value":"Method"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We compare the speed and accuracy of KDE methods for both univariate and bivariate visualizations.\nTo measure accuracy, we compare against a direct calculation approach.\nAs pixel-based visualizations are inherently discrete, we compute "},{"type":"component","name":"quote","properties":{"class":{"type":"value","value":"single"}},"children":[{"type":"textnode","value":"ground truth"}]},{"type":"textnode","value":" density estimates at the per-pixel level.\nWe treat each pixel as a bin and calculate the probability mass it contains, which is the difference in the KDE cumulative distribution function between the ending and starting bin boundaries.\nWe then compare these values to estimates from approximation methods.\nFor the approximate methods, we linearly interpolate results across the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" bins to produce pixel-level estimates; this matches the standard plotting method of connecting density measurement points with line segments."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"To evaluate accuracy in a visualization context, we consider how density plots are presented.\nIt is common to use a linear scale with a domain that ranges from zero to the maximum density.\nTo mirror this, prior to comparison we separately scale the density estimates, dividing each by its maximum value.\nWe then multiply by a factor of 100, so that estimates lie on a [0, 100] scale.\nThis provides an interpretable measure of error: discrepancies between methods correspond to the number of pixels difference in a 100 pixel-tall chart (a reasonable size for showing distributions, including violin plots), and simultaneously conveys a percentage difference."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We report the maximum ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"L_\\infty"}}},{"type":"textnode","value":") error, indicating the most "},{"type":"component","name":"quote","properties":{"class":{"type":"value","value":"single"}},"children":[{"type":"textnode","value":"glaring"}]},{"type":"textnode","value":" mistake a method makes; root-mean-square error gives qualitatively similar results.\nFor 1D estimation we compare to ground truth estimates for a 1024 pixel wide chart.\nFor 2D estimation we compare to ground truth for a 512 "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\times"}}},{"type":"textnode","value":" 512 heatmap.\nBoth were chosen to align with common resolutions and computation constraints."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Automatic bandwidth selection for kernel density estimation has received a great deal of attention "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Scott:1992"},"index":{"type":"value","value":21}}},{"type":"textnode","value":", "},{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Sheather:1991"},"index":{"type":"value","value":22}}}]},{"type":"textnode","value":".\nTo contextualize our results, we use Scott’s normal reference density (NRD) rule "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Scott:1992"},"index":{"type":"value","value":21}}}]},{"type":"textnode","value":", a common default intended to minimize the asymptotic mean integrated squared error (MISE) relative to a normal distribution."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"Each method was implemented as a single-threaded routine in JavaScript for web-based visualization.\nBenchmarks were run in Node.js v15.12.0 on a 2017 MacBook Pro with a 2.9 GHz Intel Core i7 processor.\nWe used the "},{"type":"component","name":"code","children":[{"type":"textnode","value":"performance.now"}]},{"type":"textnode","value":" method of the "},{"type":"component","name":"code","children":[{"type":"textnode","value":"perf_hooks"}]},{"type":"textnode","value":" package to measure running times over repeated runs."}]},{"type":"component","name":"h2","properties":{"id":{"type":"value","value":"results-1d-estimation"}},"children":[{"type":"textnode","value":"Results: 1D Estimation"}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"err1d_impulse"},"class":{"type":"value","value":"figure page"},"position":{"type":"value","value":"t"}},"children":[{"type":"component","name":"image","properties":{"width":{"type":"value","value":"100%"},"src":{"type":"value","value":"figures/error_1d_impulse.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"1D estimation error for a single impulse. Error (plotted on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Box filters exhibit an oscillating pattern due to bandwidth quantization; the extended box method smooths these artifacts. Deriche approximation consistently produces lower error, typically with sub-pixel accuracy. Linear binning further reduces the error rate."}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\vspace{4pt}"}]}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"err1d_penguins"},"class":{"type":"value","value":"figure page"},"position":{"type":"value","value":"t"}},"children":[{"type":"component","name":"image","properties":{"width":{"type":"value","value":"100%"},"src":{"type":"value","value":"figures/error_1d_penguins.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"1D estimation error for Gentoo penguin body mass. Error (plotted on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Dashed gray lines indicate the normal reference density (NRD) heuristic for automatic bandwidth ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":") selection "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Scott:1992"},"index":{"type":"value","value":21}}}]},{"type":"textnode","value":". The combination of linear binning and Deriche approximation consistently produces the most accurate estimates."}]}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"kde_penguins"},"class":{"type":"value","value":"figure margin"},"position":{"type":"value","value":"t"}},"children":[{"type":"component","name":"image","properties":{"src":{"type":"value","value":"figures/penguins_1d.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"KDE of Gentoo penguin body mass ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins, "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" = 50). Box filters tend to underestimate peaks and overestimate valleys, in some cases "},{"type":"component","name":"quote","properties":{"class":{"type":"value","value":"single"}},"children":[{"type":"textnode","value":"eroding"}]},{"type":"textnode","value":" local peaks (e.g., around 4.9k & 5.7k grams). Deriche approximation instead produces a pixel-perfect result."}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\vspace{8pt}"}]}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We first evaluate the KDE methods relative to an impulse: we locate a single point at "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"x = 0"}}},{"type":"textnode","value":" and perform estimation over the domain "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"[-1, 1]"}}},{"type":"textnode","value":".\nThe result should be a Gaussian distribution with mean 0 and standard deviation matching the kernel bandwidth "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":".\n"},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"kde_impulse"}}},{"type":"textnode","value":" shows the result for "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" = 0.2 and "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins (sans re-scaling).\nThe box filter methods produce perceptible errors, whereas Deriche approximation provides a pixel-perfect match to the actual distribution."}]},{"type":"component","name":"p","children":[{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"err1d_impulse"}}},{"type":"textnode","value":" presents scaled comparisons by binning scheme, bin count, and bandwidth.\nStandard box filters produce oscillating errors due to bandwidth quantization.\nThe extended box method smooths these artifacts.\nDeriche approximation consistently produces the lowest error, and notably improves with the use of linear binning."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We next examine real-world measurements from the Palmer Penguins dataset "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Horst:2020"},"index":{"type":"value","value":10}}}]},{"type":"textnode","value":".\nWe estimate densities for penguin body mass (in grams) for "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" = 123 Gentoo penguins on the domain "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"[0, 7000]"}}},{"type":"textnode","value":".\n"},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"err1d_penguins"}}},{"type":"textnode","value":" shows maximum estimation errors.\nWe again see that the combination of Deriche approximation and linear binning produces the best results, often with sub-pixel error.\n"},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"kde_penguins"}}},{"type":"textnode","value":" shows a subset of the visualized density.\nThe box filter methods again produce perceptible deviations, which in multiple instances obscure local extrema.\nDeriche approximation produces a pixel-perfect result."}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"time1d_penguins"},"class":{"type":"value","value":"figure"},"position":{"type":"value","value":"t"}},"children":[{"type":"component","name":"image","properties":{"width":{"type":"value","value":"100%"},"src":{"type":"value","value":"figures/time_1d_penguins.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"Running time of 1D estimation on resampled penguin data ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins). As "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" increases, the running time of the approximation methods is dominated by the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n)"}}},{"type":"textnode","value":" binning cost."}]}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"To assess scalability, we generate datasets of arbitrary size based on the Gentoo penguins data.\nWe first fit a kernel density estimate using direct calculation ("},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" = 200, based on the NRD value of 204.11), then sample from the resulting distribution to generate datasets ranging from "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" = 100 to "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" = 10M points.\nEach timing measurement is taken for "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins and averages runs for five bandwidths (100, 150, 200, 250, 300) centered near the original NRD value.\n"},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"time1d_penguins"}}},{"type":"textnode","value":" plots the results.\nFor small "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":", box filtering is slightly faster as it involves fewer arithmetic operations.\nAs "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"n"}}},{"type":"textnode","value":" increases, the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n)"}}},{"type":"textnode","value":" binning calculation dominates and all methods exhibit similar performance."}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\place{err2d_cars}"}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\place{time2d_cars}"}]},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\place{contours}"}]},{"type":"component","name":"h2","properties":{"id":{"type":"value","value":"results-2d-estimation"}},"children":[{"type":"textnode","value":"Results: 2D Estimation"}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"err2d_cars"},"class":{"type":"value","value":"figure page"},"position":{"type":"value","value":"t"}},"children":[{"type":"component","name":"image","properties":{"width":{"type":"value","value":"100%"},"src":{"type":"value","value":"figures/error_2d_cars.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"2D estimation error for car data. Error (on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Dashed gray lines indicate the NRD "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" value. With 512 bins and linear binning, the Deriche method results in sub-pixel accuracy at all sampled bandwidths."}]}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"contours"},"class":{"type":"value","value":"figure margin"},"position":{"type":"value","value":"h!"}},"children":[{"type":"component","name":"image","properties":{"src":{"type":"value","value":"figures/cars_2d_multiples.svg"}}},{"type":"textnode","value":"\n"},{"type":"component","name":"image","properties":{"src":{"type":"value","value":"figures/cars_2d_overlap.svg"}}},{"type":"textnode","value":"\n"},{"type":"component","name":"raw","properties":{"format":{"type":"value","value":"tex"}},"children":[{"type":"textnode","value":"\\vspace{-30pt}"}]},{"type":"component","name":"caption","children":[{"type":"textnode","value":"Heatmaps and contour plots of car data (miles per gallon vs. horsepower). "},{"type":"component","name":"em","children":[{"type":"textnode","value":"Top:"}]},{"type":"textnode","value":" plots per density method. "},{"type":"component","name":"em","children":[{"type":"textnode","value":"Bottom:"}]},{"type":"textnode","value":" contour lines per method overlaid for comparison. Deriche’s approximation matches the precise density estimate. Box filters result in extra or missing contour lines and distorted shapes."}]}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"To assess bivariate estimates, we use the classic cars dataset "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"CarsData"},"index":{"type":"value","value":16}}}]},{"type":"textnode","value":" and examine the relationship between mileage and horsepower.\nWe use the same error measure.\n"},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"err2d_cars"}}},{"type":"textnode","value":" presents the error across binning and bandwidth choices (the same bandwidth is used for the x- and y-dimensions), with similar patterns as before.\nDeriche approximation with linear binning at "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"m"}}},{"type":"textnode","value":" = 512 bins produces notably low error rates."}]},{"type":"component","name":"p","children":[{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"contours"}}},{"type":"textnode","value":" shows heatmaps and contour plots for 2D density estimation, both as separate plots and as contours overlaid on a ground truth heatmap.\nWe set "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"\\sigma"}}},{"type":"textnode","value":" = 0.04 for both the x- and y-dimensions, near the NRD estimates (0.049, 0.048) for each variable.\nThe same contour thresholds – "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"[0.01, 0.08]"}}},{"type":"textnode","value":" with increments of 0.01 – are applied for each method.\nComparison of the contour plots reveals hallucinators "},{"type":"component","name":"citelist","properties":{"class":{"type":"value","value":"cite-list"}},"children":[{"type":"component","name":"citeref","properties":{"key":{"type":"value","value":"Kindlmann:2014"},"index":{"type":"value","value":12}}}]},{"type":"textnode","value":", where approximation methods produce different visual features for the same underlying data.\nThe Deriche method provides a pixel-perfect match to the true density, but the box filter methods result in different extrema as well as distorted contour shapes."}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"As shown earlier ("},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"time1d_penguins"}}},{"type":"textnode","value":"), for large datasets the running time of binned KDE methods is dominated by the "},{"type":"component","name":"math","properties":{"mode":{"type":"value","value":"inline"},"code":{"type":"value","value":"O(n)"}}},{"type":"textnode","value":" binning.\nHere we instead assess the effect of bandwidth on 2D estimation time, shown in "},{"type":"component","name":"crossref","properties":{"type":{"type":"value","value":"fig"},"xref":{"type":"value","value":"time2d_cars"}}},{"type":"textnode","value":".\nAt low bandwidths, standard box filtering is fastest due to fewer operations per bin.\nHowever, both box filter methods become slower at larger bandwidths due to the need to enlarge the underlying grid.\nThis overhead is exacerbated for 2D estimation, as the number of expanded cells multiply across grid rows and columns.\nIn contrast the Deriche method is stable across bandwidths as it does not require grid extensions, with performance matching or exceeding the other methods for bandwidths at or above the NRD bandwidth suggestion."}]},{"type":"component","name":"figure","properties":{"id":{"type":"value","value":"time2d_cars"},"class":{"type":"value","value":"figure"},"position":{"type":"value","value":"h!"}},"children":[{"type":"component","name":"image","properties":{"width":{"type":"value","value":"100%"},"src":{"type":"value","value":"figures/time_2d_cars.svg"}}},{"type":"component","name":"caption","children":[{"type":"textnode","value":"Running time of 2D estimation on car data, by bandwidth. At low bandwidths, Deriche’s method is slightly slower due to more arithmetic operations. As the bandwidth increases, the box filters require larger grids, leading to longer running times."}]}]},{"type":"component","name":"h1","properties":{"id":{"type":"value","value":"conclusion"}},"children":[{"type":"textnode","value":"Conclusion"}]},{"type":"component","name":"p","children":[{"type":"textnode","value":"We survey approaches for KDE, finding that a combination of linear binning and Deriche approximation results in fast, linear-time performance and excellent accuracy at all but the smallest bandwidths.\nThough limited to Gaussian kernels only, this approach provides fast and accurate KDE for the tested univariate and bivariate visualizations.\nOur implementation and benchmarks, including additional error metrics, are available as open source software ("},{"type":"component","name":"link","properties":{"class":{"type":"value","value":"uri"},"href":{"type":"value","value":"https://github.com/uwdata/fast-kde"}},"children":[{"type":"textnode","value":"https://github.com/uwdata/fast-kde"}]},{"type":"textnode","value":") and we intend to integrate this method into existing web-based visualization libraries."}]},{"type":"component","name":"acknowledgments","children":[{"type":"component","name":"p","children":[{"type":"textnode","value":"We thank the UW Interactive Data Lab and anonymous reviewers.\nThe work was supported by a Moore Foundation software grant."}]}]},{"type":"component","name":"references","children":[{"type":"component","name":"ol","properties":{"class":{"type":"value","value":"references"}},"children":[{"type":"component","name":"li","children":[{"type":"textnode","value":"Bostock, M., Ogievetsky, V., & Heer, J. (2011). D3: Data-Driven Documents. IEEE Transactions on Visualization and Computer Graphics, 17(12), 2301–2309. https://doi.org/10.1109/TVCG.2011.185"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Bullmann, M., Fetzer, T., Ebner, F., Deinzer, F., & Grzegorzek, M. (2018). Fast Kernel Density Estimation Using Gaussian Filter Approximation. Proc. International Conference on Information Fusion (FUSION), 1233–1240. https://doi.org/10.23919/ICIF.2018.8455686"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Correll, M., & Gleicher, M. (2014). Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2142–2151. https://doi.org/10.1109/TVCG.2014.2346298"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Deriche, R. (1990). Fast Algorithms for Low-Level Vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(1), 78–87. https://doi.org/10.1109/34.41386"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Deriche, R. (1993). Recursively implementing the Gaussian and its derivatives [Techreport]. INRIA. http://hal.inria.fr/docs/00/07/47/78/PDF/RR-1893.svg"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Getreuer, P. (2013). A Survey of Gaussian Convolution Algorithms. Image Processing On Line, 3, 286–310. https://doi.org/10.5201/ipol.2013.87"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Gray, A. G., & Moore, A. W. (2003). Nonparametric density estimation: Toward computational tractability. Proc. 2003 SIAM International Conference on Data Mining, 203–211. https://doi.org/10.1137/1.9781611972733.19"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Gwosdek, P., Grewenig, S., Bruhn, A., & Weickert, J. (2011). Theoretical foundations of Gaussian convolution by extended box filtering. Proc. International Conference on Scale Space and Variational Methods in Computer Vision, 447–458. https://doi.org/10.1007/978-3-642-24785-9_38"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Hintze, J. L., & Nelson, R. D. (1998). Violin Plots: A Box Plot-Density Trace Synergism. The American Statistician, 52(2), 181–184. https://doi.org/10.1080/00031305.1998.10480559"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Horst, A. M., Hill, A. P., & Gorman, K. B. (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Jones, M. C., & Lotwick, H. W. (1983). On the errors involved in computing the empirical characteristic function. Journal of Statistical Computation and Simulation, 17(2), 133–149. https://doi.org/10.1080/00949658308810650"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Kindlmann, G., & Scheidegger, C. (2014). An Algebraic Process for Visualization Design. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2181–2190. https://doi.org/10.1109/TVCG.2014.2346325"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Lorensen, W. E., & Cline, H. E. (1987). Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH Computer Graphics, 21(4), 163–169. https://doi.org/10.1145/37402.37422"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Parzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33(3), 1065–1076. https://doi.org/10.1214/aoms/1177704472"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. https://doi.org/10.5555/1953048.2078195"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Ramos, E., & Donoho, D. (1983). ASA Data Exposition Dataset. http://stat-computing.org/dataexpo/1983.html. http://stat-computing.org/dataexpo/1983.html"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Rosenblatt, M. (1956). Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics, 27(3), 832–837. https://doi.org/10.1214/aoms/1177728190"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Satyanarayan, A., Russell, R., Hoffswell, J., & Heer, J. (2015). Reactive Vega: A streaming dataflow architecture for declarative interactive visualization. IEEE Transactions on Visualization and Computer Graphics, 22(1), 659–668. https://doi.org/10.1109/TVCG.2015.2467091"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Satyanarayan, A., Moritz, D., Wongsuphasawat, K., & Heer, J. (2016). Vega-Lite: A grammar of interactive graphics. IEEE Transactions on Visualization and Computer Graphics, 23(1), 341–350. https://doi.org/10.1109/TVCG.2016.2599030"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Scott, D. W., & Sheather, S. J. (1985). Kernel density estimation with binned data. Communications in Statistics - Theory and Methods, 14(6), 1353–1359. https://doi.org/10.1080/03610928508828980"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Scott, D. W. (1992). Multivariate Density Estimation: Theory, Practice, and Visualization. John Wiley & Sons. https://doi.org/10.1002/9780470316849"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Sheather, S. J., & Jones, M. C. (1991). A reliable data-based bandwidth selection method for kernel density estimation. Journal of the Royal Statistical Society, Series B (Methodological), 53(3), 683–690. https://doi.org/10.1111/j.2517-6161.1991.tb01857.x"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Silverman, B. W. (1982). Algorithm AS 176: Kernel density estimation using the fast Fourier transform. Journal of the Royal Statistical Society, Series C (Applied Statistics), 31(1), 93–99. https://doi.org/10.2307/2347084"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Venables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S. Springer. https://doi.org/10.1007/978-0-387-21706-2"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Wand, M. P. (1994). Fast Computation of Multivariate Kernel Estimators. Journal of Statistical Computation and Simulation, 3(4), 433–445. https://doi.org/10.2307/1390904"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Wells, W. M. (1986). Efficient synthesis of Gaussian filters by cascaded uniform filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 8(2), 234–239. https://doi.org/10.1109/TPAMI.1986.4767776"}]},{"type":"component","name":"li","children":[{"type":"textnode","value":"Wickham, H. (2009). ggplot2: Elegant Graphics for Data Analysis. Springer. https://doi.org/10.1007/978-0-387-98141-3"}]}]}]}]},"data":{"citations":{"bibtex":["@article{Bostock:2011,\n\tauthor = {Bostock, Michael and Ogievetsky, Vadim and Heer, Jeffrey},\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics},\n\tnumber = {12},\n\tyear = {2011},\n\tpages = {2301--2309},\n\tpublisher = {IEEE},\n\ttitle = {D3: Data-{Driven} {Documents}},\n\tvolume = {17},\n}","@inproceedings{Bullman:2018,\n\tauthor = {Bullmann, M. and Fetzer, T. and Ebner, F. and Deinzer, F. and Grzegorzek, M.},\n\tbooktitle = {Proc. {International} {Conference} on {Information} {Fusion} ({FUSION})},\n\tyear = {2018},\n\tpages = {1233--1240},\n\ttitle = {Fast {Kernel} {Density} {Estimation} {Using} {Gaussian} {Filter} {Approximation}},\n}","@article{Correll:2014,\n\tauthor = {Correll, Michael and Gleicher, Michael},\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics},\n\tnumber = {12},\n\tyear = {2014},\n\tpages = {2142--2151},\n\tpublisher = {IEEE},\n\ttitle = {Error {Bars} {Considered} {Harmful}: Exploring {Alternate} {Encodings} for {Mean} and {Error}},\n\tvolume = {20},\n}","@article{Deriche:1990,\n\tauthor = {Deriche, R.},\n\tjournal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n\tnumber = {1},\n\tyear = {1990},\n\tpages = {78--87},\n\ttitle = {Fast {Algorithms} for {Low}-{Level} {Vision}},\n\tvolume = {12},\n}","@techreport{Deriche:1993,\n\tauthor = {Deriche, R.},\n\tyear = {1993},\n\tinstitution = {INRIA},\n\ttitle = {Recursively implementing the {Gaussian} and its derivatives},\n\ttype = {techreport},\n}","@article{Getreuer:2013,\n\tauthor = {Getreuer, P.},\n\tjournal = {Image Processing On Line},\n\tyear = {2013},\n\tpages = {286--310},\n\ttitle = {A {Survey} of {Gaussian} {Convolution} {Algorithms}},\n\tvolume = {3},\n}","@inproceedings{Gray:2003,\n\tauthor = {Gray, Alexander G and Moore, Andrew W},\n\tbooktitle = {Proc. 2003 {SIAM} {International} {Conference} on {Data} {Mining}},\n\tyear = {2003},\n\tpages = {203--211},\n\ttitle = {Nonparametric density estimation: Toward computational tractability},\n}","@inproceedings{Gwosdek:2011,\n\tauthor = {Gwosdek, P. and Grewenig, S. and Bruhn, A. and Weickert, J.},\n\tbooktitle = {Proc. {International} {Conference} on {Scale} {Space} and {Variational} {Methods} in {Computer} {Vision}},\n\tyear = {2011},\n\tpages = {447--458},\n\ttitle = {Theoretical foundations of {Gaussian} convolution by extended box filtering},\n}","@article{Hintzel:1998,\n\tauthor = {Hintze, Jerry L and Nelson, Ray D},\n\tjournal = {The American Statistician},\n\tnumber = {2},\n\tyear = {1998},\n\tpages = {181--184},\n\tpublisher = {Taylor & Francis},\n\ttitle = {Violin {Plots}: A {Box} {Plot}-{Density} {Trace} {Synergism}},\n\tvolume = {52},\n}","@techreport{Horst:2020,\n\tauthor = {Horst, Allison Marie and Hill, Alison Presmanes and Gorman, Kristen B},\n\tyear = {2020},\n\tnote = {R package version 0.1.0},\n\ttitle = {palmerpenguins: Palmer {Archipelago} ({Antarctica}) penguin data},\n\thowpublished = {https://allisonhorst.github.io/palmerpenguins/},\n}","@article{Jones:1983,\n\tauthor = {Jones, M. C. and Lotwick, H. W.},\n\tjournal = {Journal of Statistical Computation and Simulation},\n\tnumber = {2},\n\tyear = {1983},\n\tpages = {133--149},\n\ttitle = {On the errors involved in computing the empirical characteristic function},\n\tvolume = {17},\n}","@article{Kindlmann:2014,\n\tauthor = {Kindlmann, Gordon and Scheidegger, Carlos},\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics},\n\tnumber = {12},\n\tyear = {2014},\n\tpages = {2181--2190},\n\ttitle = {An {Algebraic} {Process} for {Visualization} {Design}},\n\tvolume = {20},\n}","@article{Lorensen:1987:MCA,\n\tauthor = {Lorensen, William E. and Cline, Harvey E.},\n\tjournal = {SIGGRAPH Computer Graphics},\n\tnumber = {4},\n\tyear = {1987},\n\tmonth = {8},\n\tpages = {163--169},\n\ttitle = {Marching {Cubes}: A {High} {Resolution} 3D {Surface} {Construction} {Algorithm}},\n\tvolume = {21},\n}","@article{Parzen:1962,\n\tauthor = {Parzen, Emanuel},\n\tjournal = {The Annals of Mathematical Statistics},\n\tnumber = {3},\n\tyear = {1962},\n\tpages = {1065 -- 1076},\n\tpublisher = {Institute of Mathematical Statistics},\n\ttitle = {On {Estimation} of a {Probability} {Density} {Function} and {Mode}},\n\tvolume = {33},\n}","@article{ScikitLearn,\n\tauthor = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n\tjournal = {Journal of Machine Learning Research},\n\tyear = {2011},\n\tpages = {2825--2830},\n\ttitle = {Scikit-learn: Machine {Learning} in {Python}},\n\tvolume = {12},\n}","@misc{CarsData,\n\tauthor = {Ramos, Ernesto and Donoho, David},\n\tyear = {1983},\n\tpublisher = {http://stat-computing.org/dataexpo/1983.html},\n\ttitle = {ASA {Data} {Exposition} {Dataset}},\n}","@article{Rosenblatt:1956,\n\tauthor = {Rosenblatt, Murray},\n\tjournal = {The Annals of Mathematical Statistics},\n\tnumber = {3},\n\tyear = {1956},\n\tpages = {832--837},\n\tpublisher = {Institute of Mathematical Statistics},\n\ttitle = {Remarks on {Some} {Nonparametric} {Estimates} of a {Density} {Function}},\n\tvolume = {27},\n}","@article{Satyanarayan:2015,\n\tauthor = {Satyanarayan, Arvind and Russell, Ryan and Hoffswell, Jane and Heer, Jeffrey},\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics},\n\tnumber = {1},\n\tyear = {2015},\n\tpages = {659--668},\n\tpublisher = {IEEE},\n\ttitle = {Reactive {Vega}: A streaming dataflow architecture for declarative interactive visualization},\n\tvolume = {22},\n}","@article{Satyanarayan:2016,\n\tauthor = {Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics},\n\tnumber = {1},\n\tyear = {2016},\n\tpages = {341--350},\n\tpublisher = {IEEE},\n\ttitle = {Vega-{Lite}: A grammar of interactive graphics},\n\tvolume = {23},\n}","@article{Scott:1985,\n\tauthor = {Scott, D. W. and Sheather, S. J.},\n\tjournal = {Communications in Statistics - Theory and Methods},\n\tnumber = {6},\n\tyear = {1985},\n\tpages = {1353--1359},\n\ttitle = {Kernel density estimation with binned data},\n\tvolume = {14},\n}","@book{Scott:1992,\n\tauthor = {Scott, David W},\n\tyear = {1992},\n\tpublisher = {John Wiley & Sons},\n\ttitle = {Multivariate {Density} {Estimation}: Theory, {Practice}, and {Visualization}},\n}","@article{Sheather:1991,\n\tauthor = {Sheather, S. J. and Jones, M. C.},\n\tjournal = {Journal of the Royal Statistical Society, Series B (Methodological)},\n\tnumber = {3},\n\tyear = {1991},\n\tpages = {683--690},\n\ttitle = {A reliable data-based bandwidth selection method for kernel density estimation},\n\tvolume = {53},\n}","@article{Silverman:1982,\n\tauthor = {Silverman, B. W.},\n\tjournal = {Journal of the Royal Statistical Society, Series C (Applied Statistics)},\n\tnumber = {1},\n\tyear = {1982},\n\tpages = {93--99},\n\ttitle = {Algorithm {AS} 176: Kernel density estimation using the fast {Fourier} transform},\n\tvolume = {31},\n}","@book{MASS:2002,\n\tauthor = {Venables, W. N. and Ripley, B. D.},\n\tyear = {2002},\n\tpublisher = {Springer},\n\ttitle = {Modern applied statistics with {S}},\n}","@article{Wand:1994,\n\tauthor = {Wand, M. P.},\n\tjournal = {Journal of Statistical Computation and Simulation},\n\tnumber = {4},\n\tyear = {1994},\n\tpages = {433--445},\n\ttitle = {Fast {Computation} of {Multivariate} {Kernel} {Estimators}},\n\tvolume = {3},\n}","@article{Wells:1986,\n\tauthor = {Wells, W. M.},\n\tjournal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n\tnumber = {2},\n\tyear = {1986},\n\tpages = {234--239},\n\ttitle = {Efficient synthesis of {Gaussian} filters by cascaded uniform filters},\n\tvolume = {8},\n}","@book{Wickham:2009,\n\tauthor = {Wickham, Hadley},\n\tyear = {2009},\n\tpublisher = {Springer},\n\ttitle = {ggplot2: Elegant {Graphics} for {Data} {Analysis}},\n}"],"csl":[{"container-title":"IEEE Transactions on Visualization and Computer Graphics","author":[{"given":"Michael","family":"Bostock"},{"given":"Vadim","family":"Ogievetsky"},{"given":"Jeffrey","family":"Heer"}],"DOI":"10.1109/TVCG.2011.185","type":"article-journal","id":"Bostock:2011","citation-key":"Bostock:2011","issue":"12","issued":{"date-parts":[[2011]]},"page":"2301-2309","publisher":"IEEE","title":"D3: Data-Driven Documents","volume":"17"},{"author":[{"given":"M.","family":"Bullmann"},{"given":"T.","family":"Fetzer"},{"given":"F.","family":"Ebner"},{"given":"F.","family":"Deinzer"},{"given":"M.","family":"Grzegorzek"}],"container-title":"Proc. International Conference on Information Fusion (FUSION)","DOI":"10.23919/ICIF.2018.8455686","type":"paper-conference","id":"Bullman:2018","citation-key":"Bullman:2018","issued":{"date-parts":[[2018]]},"page":"1233-1240","title":"Fast Kernel Density Estimation Using Gaussian Filter Approximation"},{"container-title":"IEEE Transactions on Visualization and Computer Graphics","author":[{"given":"Michael","family":"Correll"},{"given":"Michael","family":"Gleicher"}],"DOI":"10.1109/TVCG.2014.2346298","type":"article-journal","id":"Correll:2014","citation-key":"Correll:2014","issue":"12","issued":{"date-parts":[[2014]]},"page":"2142-2151","publisher":"IEEE","title":"Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error","volume":"20"},{"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","author":[{"given":"R.","family":"Deriche"}],"DOI":"10.1109/34.41386","type":"article-journal","id":"Deriche:1990","citation-key":"Deriche:1990","issue":"1","issued":{"date-parts":[[1990]]},"page":"78-87","title":"Fast Algorithms for Low-Level Vision","volume":"12"},{"author":[{"given":"R.","family":"Deriche"}],"type":"report","genre":"techreport","id":"Deriche:1993","citation-key":"Deriche:1993","issued":{"date-parts":[[1993]]},"publisher":"INRIA","title":"Recursively implementing the Gaussian and its derivatives","URL":"http://hal.inria.fr/docs/00/07/47/78/PDF/RR-1893.svg"},{"container-title":"Image Processing On Line","author":[{"given":"P.","family":"Getreuer"}],"DOI":"10.5201/ipol.2013.87","type":"article-journal","id":"Getreuer:2013","citation-key":"Getreuer:2013","issued":{"date-parts":[[2013]]},"page":"286-310","title":"A Survey of Gaussian Convolution Algorithms","volume":"3"},{"author":[{"given":"Alexander G","family":"Gray"},{"given":"Andrew W","family":"Moore"}],"container-title":"Proc. 2003 SIAM International Conference on Data Mining","DOI":"10.1137/1.9781611972733.19","type":"paper-conference","id":"Gray:2003","citation-key":"Gray:2003","issued":{"date-parts":[[2003]]},"page":"203-211","title":"Nonparametric density estimation: Toward computational tractability"},{"author":[{"given":"P.","family":"Gwosdek"},{"given":"S.","family":"Grewenig"},{"given":"A.","family":"Bruhn"},{"given":"J.","family":"Weickert"}],"container-title":"Proc. International Conference on Scale Space and Variational Methods in Computer Vision","DOI":"10.1007/978-3-642-24785-9_38","type":"paper-conference","id":"Gwosdek:2011","citation-key":"Gwosdek:2011","issued":{"date-parts":[[2011]]},"page":"447-458","title":"Theoretical foundations of Gaussian convolution by extended box filtering"},{"container-title":"The American Statistician","author":[{"given":"Jerry L","family":"Hintze"},{"given":"Ray D","family":"Nelson"}],"DOI":"10.1080/00031305.1998.10480559","type":"article-journal","id":"Hintzel:1998","citation-key":"Hintzel:1998","issue":"2","issued":{"date-parts":[[1998]]},"page":"181-184","publisher":"Taylor & Francis","title":"Violin Plots: A Box Plot-Density Trace Synergism","volume":"52"},{"author":[{"given":"Allison Marie","family":"Horst"},{"given":"Alison Presmanes","family":"Hill"},{"given":"Kristen B","family":"Gorman"}],"DOI":"10.5281/zenodo.3960218","type":"report","id":"Horst:2020","citation-key":"Horst:2020","issued":{"date-parts":[[2020]]},"note":"R package version 0.1.0","title":"palmerpenguins: Palmer Archipelago (Antarctica) penguin data","URL":"https://allisonhorst.github.io/palmerpenguins/"},{"container-title":"Journal of Statistical Computation and Simulation","author":[{"given":"M. C.","family":"Jones"},{"given":"H. W.","family":"Lotwick"}],"DOI":"10.1080/00949658308810650","type":"article-journal","id":"Jones:1983","citation-key":"Jones:1983","issue":"2","issued":{"date-parts":[[1983]]},"page":"133-149","title":"On the errors involved in computing the empirical characteristic function","volume":"17"},{"container-title":"IEEE Transactions on Visualization and Computer Graphics","author":[{"given":"Gordon","family":"Kindlmann"},{"given":"Carlos","family":"Scheidegger"}],"DOI":"10.1109/TVCG.2014.2346325","type":"article-journal","id":"Kindlmann:2014","citation-key":"Kindlmann:2014","issue":"12","issued":{"date-parts":[[2014]]},"page":"2181-2190","title":"An Algebraic Process for Visualization Design","volume":"20"},{"container-title":"SIGGRAPH Computer Graphics","author":[{"given":"William E.","family":"Lorensen"},{"given":"Harvey E.","family":"Cline"}],"DOI":"10.1145/37402.37422","type":"article-journal","id":"Lorensen:1987:MCA","citation-key":"Lorensen:1987:MCA","issue":"4","issued":{"date-parts":[[1987,8]]},"page":"163-169","title":"Marching Cubes: A High Resolution 3D Surface Construction Algorithm","volume":"21"},{"container-title":"The Annals of Mathematical Statistics","author":[{"given":"Emanuel","family":"Parzen"}],"DOI":"10.1214/aoms/1177704472","type":"article-journal","id":"Parzen:1962","citation-key":"Parzen:1962","issue":"3","issued":{"date-parts":[[1962]]},"page":"1065 - 1076","publisher":"Institute of Mathematical Statistics","title":"On Estimation of a Probability Density Function and Mode","volume":"33"},{"container-title":"Journal of Machine Learning Research","author":[{"given":"F.","family":"Pedregosa"},{"given":"G.","family":"Varoquaux"},{"given":"A.","family":"Gramfort"},{"given":"V.","family":"Michel"},{"given":"B.","family":"Thirion"},{"given":"O.","family":"Grisel"},{"given":"M.","family":"Blondel"},{"given":"P.","family":"Prettenhofer"},{"given":"R.","family":"Weiss"},{"given":"V.","family":"Dubourg"},{"given":"J.","family":"Vanderplas"},{"given":"A.","family":"Passos"},{"given":"D.","family":"Cournapeau"},{"given":"M.","family":"Brucher"},{"given":"M.","family":"Perrot"},{"given":"E.","family":"Duchesnay"}],"DOI":"10.5555/1953048.2078195","type":"article-journal","id":"ScikitLearn","citation-key":"ScikitLearn","issued":{"date-parts":[[2011]]},"page":"2825-2830","title":"Scikit-learn: Machine Learning in Python","volume":"12"},{"author":[{"given":"Ernesto","family":"Ramos"},{"given":"David","family":"Donoho"}],"type":"document","id":"CarsData","citation-key":"CarsData","issued":{"date-parts":[[1983]]},"publisher":"http://stat-computing.org/dataexpo/1983.html","title":"ASA Data Exposition Dataset","URL":"http://stat-computing.org/dataexpo/1983.html"},{"container-title":"The Annals of Mathematical Statistics","author":[{"given":"Murray","family":"Rosenblatt"}],"DOI":"10.1214/aoms/1177728190","type":"article-journal","id":"Rosenblatt:1956","citation-key":"Rosenblatt:1956","issue":"3","issued":{"date-parts":[[1956]]},"page":"832-837","publisher":"Institute of Mathematical Statistics","title":"Remarks on Some Nonparametric Estimates of a Density Function","volume":"27"},{"container-title":"IEEE Transactions on Visualization and Computer Graphics","author":[{"given":"Arvind","family":"Satyanarayan"},{"given":"Ryan","family":"Russell"},{"given":"Jane","family":"Hoffswell"},{"given":"Jeffrey","family":"Heer"}],"DOI":"10.1109/TVCG.2015.2467091","type":"article-journal","id":"Satyanarayan:2015","citation-key":"Satyanarayan:2015","issue":"1","issued":{"date-parts":[[2015]]},"page":"659-668","publisher":"IEEE","title":"Reactive Vega: A streaming dataflow architecture for declarative interactive visualization","volume":"22"},{"container-title":"IEEE Transactions on Visualization and Computer Graphics","author":[{"given":"Arvind","family":"Satyanarayan"},{"given":"Dominik","family":"Moritz"},{"given":"Kanit","family":"Wongsuphasawat"},{"given":"Jeffrey","family":"Heer"}],"DOI":"10.1109/TVCG.2016.2599030","type":"article-journal","id":"Satyanarayan:2016","citation-key":"Satyanarayan:2016","issue":"1","issued":{"date-parts":[[2016]]},"page":"341-350","publisher":"IEEE","title":"Vega-Lite: A grammar of interactive graphics","volume":"23"},{"container-title":"Communications in Statistics - Theory and Methods","author":[{"given":"D. W.","family":"Scott"},{"given":"S. J.","family":"Sheather"}],"DOI":"10.1080/03610928508828980","type":"article-journal","id":"Scott:1985","citation-key":"Scott:1985","issue":"6","issued":{"date-parts":[[1985]]},"page":"1353-1359","title":"Kernel density estimation with binned data","volume":"14"},{"author":[{"given":"David W","family":"Scott"}],"DOI":"10.1002/9780470316849","type":"book","id":"Scott:1992","citation-key":"Scott:1992","issued":{"date-parts":[[1992]]},"publisher":"John Wiley & Sons","title":"Multivariate Density Estimation: Theory, Practice, and Visualization"},{"container-title":"Journal of the Royal Statistical Society, Series B (Methodological)","author":[{"given":"S. J.","family":"Sheather"},{"given":"M. C.","family":"Jones"}],"DOI":"10.1111/j.2517-6161.1991.tb01857.x","type":"article-journal","id":"Sheather:1991","citation-key":"Sheather:1991","issue":"3","issued":{"date-parts":[[1991]]},"page":"683-690","title":"A reliable data-based bandwidth selection method for kernel density estimation","volume":"53"},{"container-title":"Journal of the Royal Statistical Society, Series C (Applied Statistics)","author":[{"given":"B. W.","family":"Silverman"}],"DOI":"10.2307/2347084","type":"article-journal","id":"Silverman:1982","citation-key":"Silverman:1982","issue":"1","issued":{"date-parts":[[1982]]},"page":"93-99","title":"Algorithm AS 176: Kernel density estimation using the fast Fourier transform","volume":"31"},{"author":[{"given":"W. N.","family":"Venables"},{"given":"B. D.","family":"Ripley"}],"DOI":"10.1007/978-0-387-21706-2","type":"book","id":"MASS:2002","citation-key":"MASS:2002","issued":{"date-parts":[[2002]]},"publisher":"Springer","title":"Modern applied statistics with S"},{"container-title":"Journal of Statistical Computation and Simulation","author":[{"given":"M. P.","family":"Wand"}],"DOI":"10.2307/1390904","type":"article-journal","id":"Wand:1994","citation-key":"Wand:1994","issue":"4","issued":{"date-parts":[[1994]]},"page":"433-445","title":"Fast Computation of Multivariate Kernel Estimators","volume":"3"},{"container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","author":[{"given":"W. M.","family":"Wells"}],"DOI":"10.1109/TPAMI.1986.4767776","type":"article-journal","id":"Wells:1986","citation-key":"Wells:1986","issue":"2","issued":{"date-parts":[[1986]]},"page":"234-239","title":"Efficient synthesis of Gaussian filters by cascaded uniform filters","volume":"8"},{"author":[{"given":"Hadley","family":"Wickham"}],"DOI":"10.1007/978-0-387-98141-3","type":"book","id":"Wickham:2009","citation-key":"Wickham:2009","issued":{"date-parts":[[2009]]},"publisher":"Springer","title":"ggplot2: Elegant Graphics for Data Analysis"}],"data":[{"id":"Bostock:2011","doi":"10.1109/TVCG.2011.185","s2id":"4f9630d72ae64e50b2cc110e7b10834e965e86fe","year":2011,"author":[{"given":"Michael","family":"Bostock"},{"given":"Vadim","family":"Ogievetsky"},{"given":"Jeffrey","family":"Heer"}],"title":"D3: Data-Driven Documents","venue":"IEEE Transactions on Visualization and Computer Graphics","url":"https://www.semanticscholar.org/paper/4f9630d72ae64e50b2cc110e7b10834e965e86fe","abstract":"Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-specific abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components. Immediate evaluation of operators further simplifies debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.","tldr":"This work shows how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components."},{"id":"Bullman:2018","doi":"10.23919/ICIF.2018.8455686","s2id":"fdec11d4cf92e9ab353dc9374fe62287b76b0ba2","year":2018,"author":[{"given":"M.","family":"Bullmann"},{"given":"T.","family":"Fetzer"},{"given":"F.","family":"Ebner"},{"given":"F.","family":"Deinzer"},{"given":"M.","family":"Grzegorzek"}],"title":"Fast Kernel Density Estimation Using Gaussian Filter Approximation","venue":"Proc. International Conference on Information Fusion (FUSION)","url":"https://www.semanticscholar.org/paper/fdec11d4cf92e9ab353dc9374fe62287b76b0ba2","abstract":"It is common practice to use a sample-based representation to solve problems having a probabilistic interpretation. In many real world scenarios one is then interested in finding a best estimate of the underlying problem, e.g. the position of a robot. This is often done by means of simple parametric point estimators, providing the sample statistics. However, in complex scenarios this frequently results in a poor representation, due to multimodal densities and limited sample sizes. Recovering the probability density function using a kernel density estimation yields a promising approach to solve the state estimation problem i. e. finding the “real” most probable state, but comes with high computational costs. Especially in time critical and time sequential scenarios, this turns out to be impractical. Therefore, this work uses techniques from digital signal processing in the context of estimation theory, to allow rapid computations of kernel density estimates. The gains in computational efficiency are realized by substituting the Gaussian filter with an approximate filter based on the box filter. Our approach outperforms other state of the art solutions, due to a fully linear complexity and a negligible overhead, even for small sample sets. Finally, our findings are evaluated and tested within a real world sensor fusion system.","tldr":"This work uses techniques from digital signal processing in the context of estimation theory, to allow rapid computations of kernel density estimates, and outperforms other state of the art solutions, due to a fully linear complexity and a negligible overhead, even for small sample sets."},{"id":"Correll:2014","doi":"10.1109/TVCG.2014.2346298","s2id":"074ec716e8c481514f0c15e2714306b444dda0a1","year":2014,"author":[{"given":"Michael","family":"Correll"},{"given":"Michael","family":"Gleicher"}],"title":"Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error","venue":"IEEE Transactions on Visualization and Computer Graphics","url":"https://www.semanticscholar.org/paper/074ec716e8c481514f0c15e2714306b444dda0a1","abstract":"When making an inference or comparison with uncertain, noisy, or incomplete data, measurement error and confidence intervals can be as important for judgment as the actual mean values of different groups. These often misunderstood statistical quantities are frequently represented by bar charts with error bars. This paper investigates drawbacks with this standard encoding, and considers a set of alternatives designed to more effectively communicate the implications of mean and error data to a general audience, drawing from lessons learned from the use of visual statistics in the information visualization community. We present a series of crowd-sourced experiments that confirm that the encoding of mean and error significantly changes how viewers make decisions about uncertain data. Careful consideration of design tradeoffs in the visual presentation of data results in human reasoning that is more consistently aligned with statistical inferences. We suggest the use of gradient plots (which use transparency to encode uncertainty) and violin plots (which use width) as better alternatives for inferential tasks than bar charts with error bars.","tldr":"It is confirmed that the encoding of mean and error significantly changes how viewers make decisions about uncertain data, and the use of gradient plots and violin plots are suggested as better alternatives for inferential tasks than bar charts with error bars."},{"id":"Deriche:1990","doi":"10.1109/34.41386","year":1990,"author":[{"given":"R.","family":"Deriche"}],"title":"Fast Algorithms for Low-Level Vision","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence"},{"id":"Deriche:1993","year":1993,"author":[{"given":"R.","family":"Deriche"}],"title":"Recursively implementing the Gaussian and its derivatives","url":"http://hal.inria.fr/docs/00/07/47/78/PDF/RR-1893.svg"},{"id":"Getreuer:2013","doi":"10.5201/ipol.2013.87","s2id":"f99649f6002ad57110bd5d3db93fa335924edc24","year":2013,"author":[{"given":"P.","family":"Getreuer"}],"title":"A Survey of Gaussian Convolution Algorithms","venue":"Image Processing On Line","url":"https://www.semanticscholar.org/paper/f99649f6002ad57110bd5d3db93fa335924edc24","abstract":"Gaussian convolution is a common operation and building block for algorithms in signal and image processing. Consequently, its ecient computation is important, and many fast approximations have been proposed. In this survey, we discuss approximate Gaussian convolution based on nite impulse response lters, DFT and DCT based convolution, box lters, and several recursive lters. Since boundary handling is sometimes overlooked in the original works, we pay particular attention to develop it here. We perform numerical experiments to compare the speed and quality of the algorithms.","tldr":"This survey discusses approximate Gaussian convolution based on nite impulse response lters, DFT and DCT based convolution, box lter, and several recursive lters and pays particular attention to boundary handling."},{"id":"Gray:2003","doi":"10.1137/1.9781611972733.19","s2id":"7550b1c4abfd8fb046847f20cac360217057c47a","year":2003,"author":[{"given":"Alexander G","family":"Gray"},{"given":"Andrew W","family":"Moore"}],"title":"Nonparametric density estimation: Toward computational tractability","venue":"Proc. 2003 SIAM International Conference on Data Mining","url":"https://www.semanticscholar.org/paper/7550b1c4abfd8fb046847f20cac360217057c47a","abstract":"Density estimation is a core operation of virtually all probabilistic learning methods (as opposed to discriminative methods). Approaches to density estimation can be divided into two principal classes, parametric methods, such as Bayesian networks, and nonparametric methods such as kernel density estimation and smoothing splines. While neither choice should be universally preferred for all situations, a well-known benefit of nonparametric methods is their ability to achieve estimation optimality for ANY input distribution as more data are observed, a property that no model with a parametric assumption can have, and one of great importance in exploratory data analysis and mining where the underlying distribution is decidedly unknown. To date, however, despite a wealth of advanced underlying statistical theory, the use of nonparametric methods has been limited by their computational intractibility for all but the smallest datasets. In this paper, we present an algorithm for kernel density estimation, the chief nonparametric approach, which is dramatically faster than previous algorithmic approaches in terms of both dataset size and dimensionality. Furthermore, the algorithm provides arbitrarily tight accuracy guarantees, provides anytime convergence, works for all common kernel choices, and requires no parameter tuning. The algorithm is an instance of a new principle of algorithm design: multi-recursion, or higher-order","tldr":"This paper presents an algorithm for kernel density estimation, the chief nonparametric approach, which is dramatically faster than previous algorithmic approaches in terms of both dataset size and dimensionality and is an instance of a new principle of algorithm design: multi-recursion, or higher-order algorithm design."},{"id":"Gwosdek:2011","doi":"10.1007/978-3-642-24785-9_38","s2id":"9be62834011856487dbd5368d276b78ba7c652d9","year":2011,"author":[{"given":"P.","family":"Gwosdek"},{"given":"S.","family":"Grewenig"},{"given":"A.","family":"Bruhn"},{"given":"J.","family":"Weickert"}],"title":"Theoretical foundations of Gaussian convolution by extended box filtering","venue":"Proc. International Conference on Scale Space and Variational Methods in Computer Vision","url":"https://www.semanticscholar.org/paper/9be62834011856487dbd5368d276b78ba7c652d9","tldr":null},{"id":"Hintzel:1998","doi":"10.1080/00031305.1998.10480559","s2id":"c7ae5fd1d068fae58a0f7d2a5b572638db74c7fc","year":1998,"author":[{"given":"Jerry L","family":"Hintze"},{"given":"Ray D","family":"Nelson"}],"title":"Violin Plots: A Box Plot-Density Trace Synergism","venue":"The American Statistician","url":"https://www.semanticscholar.org/paper/c7ae5fd1d068fae58a0f7d2a5b572638db74c7fc","abstract":"Abstract Many modifications build on Tukey's original box plot. A proposed further adaptation, the violin plot, pools the best statistical features of alternative graphical representations of batches of data. It adds the information available from local density estimates to the basic summary statistics inherent in box plots. This marriage of summary statistics and density shape into a single plot provides a useful tool for data analysis and exploration.","tldr":"A proposed further adaptation, the violin plot, pools the best statistical features of alternative graphical representations of batches of data and adds the information available from local density estimates to the basic summary statistics inherent in box plots."},{"id":"Horst:2020","doi":"10.5281/zenodo.3960218","s2id":"d7fdefbfa895658a360efc3458721e95e6dcf861","year":2020,"author":[{"given":"Allison Marie","family":"Horst"},{"given":"Alison Presmanes","family":"Hill"},{"given":"Kristen B","family":"Gorman"}],"title":"palmerpenguins: Palmer Archipelago (Antarctica) penguin data","url":"https://www.semanticscholar.org/paper/d7fdefbfa895658a360efc3458721e95e6dcf861"},{"id":"Jones:1983","doi":"10.1080/00949658308810650","s2id":"22cd4524838f5e06b809374b6bca24ba947b1607","year":1983,"author":[{"given":"M. C.","family":"Jones"},{"given":"H. W.","family":"Lotwick"}],"title":"On the errors involved in computing the empirical characteristic function","venue":"Journal of Statistical Computation and Simulation","url":"https://www.semanticscholar.org/paper/22cd4524838f5e06b809374b6bca24ba947b1607","abstract":"This paper considers discretisation errors involved in using the Fast Fourier Transform to compute the empirical characteristic function efficiently. A simple improvement to the usual histogram discretisation scheme is shown to reduce the mean square error considerably, as the grid size tends to zero. Simulation results show that the improvement is just as good in practical cases. The theoretical results are applied to the efficient calculation of kernel density estimates, described in Silverman (1982).","tldr":"A simple improvement to the usual histogram discretisation scheme is shown to reduce the mean square error considerably, as the grid size tends to zero."},{"id":"Kindlmann:2014","doi":"10.1109/TVCG.2014.2346325","s2id":"847976a5526bbdf2f9995121e1d71de81edae776","year":2014,"author":[{"given":"Gordon","family":"Kindlmann"},{"given":"Carlos","family":"Scheidegger"}],"title":"An Algebraic Process for Visualization Design","venue":"IEEE Transactions on Visualization and Computer Graphics","url":"https://www.semanticscholar.org/paper/847976a5526bbdf2f9995121e1d71de81edae776","abstract":"We present a model of visualization design based on algebraic considerations of the visualization process. The model helps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings. The model has three components: the underlying mathematical structure of the data or object being visualized, the concrete representation of the data in a computer, and (to the extent possible) a mathematical description of how humans perceive the visualization. Because we believe the value of our model lies in its practical application, we propose three general principles for good visualization design. We work through a collection of examples where our model helps explain the known properties of existing visualizations methods, both good and not-so-good, as well as suggesting some novel methods. We describe how to use the model alongside experimental user studies, since it can help frame experiment outcomes in an actionable manner. Exploring the implications and applications of our model and its design principles should provide many directions for future visualization research.","tldr":"A model of visualization design based on algebraic considerations of the visualization process, which helps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings is presented."},{"id":"Lorensen:1987:MCA","doi":"10.1145/37402.37422","year":1987,"author":[{"given":"William E.","family":"Lorensen"},{"given":"Harvey E.","family":"Cline"}],"title":"Marching Cubes: A High Resolution 3D Surface Construction Algorithm","venue":"SIGGRAPH Computer Graphics"},{"id":"Parzen:1962","doi":"10.1214/aoms/1177704472","s2id":"de28c165623adabcdba0fdb18b65eba685aaf31d","year":1962,"author":[{"given":"Emanuel","family":"Parzen"}],"title":"On Estimation of a Probability Density Function and Mode","venue":"The Annals of Mathematical Statistics","url":"https://www.semanticscholar.org/paper/de28c165623adabcdba0fdb18b65eba685aaf31d","abstract":"Abstract : Given a sequence of independent identically distributed random variables with a common probability density function, the problem of the estimation of a probability density function and of determining the mode of a probability function are discussed. Only estimates which are consistent and asymptotically normal are constructed. (Author)"},{"id":"ScikitLearn","doi":"10.5555/1953048.2078195","s2id":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","year":2011,"author":[{"given":"F.","family":"Pedregosa"},{"given":"G.","family":"Varoquaux"},{"given":"A.","family":"Gramfort"},{"given":"V.","family":"Michel"},{"given":"B.","family":"Thirion"},{"given":"O.","family":"Grisel"},{"given":"M.","family":"Blondel"},{"given":"P.","family":"Prettenhofer"},{"given":"R.","family":"Weiss"},{"given":"V.","family":"Dubourg"},{"given":"J.","family":"Vanderplas"},{"given":"A.","family":"Passos"},{"given":"D.","family":"Cournapeau"},{"given":"M.","family":"Brucher"},{"given":"M.","family":"Perrot"},{"given":"E.","family":"Duchesnay"}],"title":"Scikit-learn: Machine Learning in Python","venue":"Journal of Machine Learning Research","url":"https://www.semanticscholar.org/paper/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","abstract":"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.","tldr":"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems, focusing on bringing machine learning to non-specialists using a general-purpose high-level language."},{"id":"CarsData","year":1983,"author":[{"given":"Ernesto","family":"Ramos"},{"given":"David","family":"Donoho"}],"title":"ASA Data Exposition Dataset","url":"http://stat-computing.org/dataexpo/1983.html"},{"id":"Rosenblatt:1956","doi":"10.1214/aoms/1177728190","s2id":"2c455f0da2bd86a9b9ea432d1485049073d7c63d","year":1956,"author":[{"given":"Murray","family":"Rosenblatt"}],"title":"Remarks on Some Nonparametric Estimates of a Density Function","venue":"The Annals of Mathematical Statistics","url":"https://www.semanticscholar.org/paper/2c455f0da2bd86a9b9ea432d1485049073d7c63d","abstract":"1. Summary. This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymp­ totic mean square error of a particular class of estimates is evaluated."},{"id":"Satyanarayan:2015","doi":"10.1109/TVCG.2015.2467091","s2id":"bab31ef6c37d54f0e1aa4666f0ccd4243354eb8f","year":2016,"author":[{"given":"Arvind","family":"Satyanarayan"},{"given":"Ryan","family":"Russell"},{"given":"Jane","family":"Hoffswell"},{"given":"Jeffrey","family":"Heer"}],"title":"Reactive Vega: A streaming dataflow architecture for declarative interactive visualization","venue":"IEEE Transactions on Visualization and Computer Graphics","url":"https://www.semanticscholar.org/paper/bab31ef6c37d54f0e1aa4666f0ccd4243354eb8f","abstract":"We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.","tldr":"Reactive Vega is presented, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization and the results of benchmark studies indicate superior interactive performance to both D3 and the original, non-reactive Vega system."},{"id":"Satyanarayan:2016","doi":"10.1109/TVCG.2016.2599030","s2id":"f3b851991e3490384100a2743aa800606990daeb","year":2018,"author":[{"given":"Arvind","family":"Satyanarayan"},{"given":"Dominik","family":"Moritz"},{"given":"Kanit","family":"Wongsuphasawat"},{"given":"Jeffrey","family":"Heer"}],"title":"Vega-Lite: A grammar of interactive graphics","venue":"IEEE Transactions on Visualization and Computer Graphics","url":"https://www.semanticscholar.org/paper/f3b851991e3490384100a2743aa800606990daeb","abstract":"We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.","tldr":"Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction, that enables rapid specification of interactive data visualizations."},{"id":"Scott:1985","doi":"10.1080/03610928508828980","s2id":"6bc7aa121a15bae1ca300e745b39a7392a510cc0","year":1985,"author":[{"given":"D. W.","family":"Scott"},{"given":"S. J.","family":"Sheather"}],"title":"Kernel density estimation with binned data","venue":"Communications in Statistics - Theory and Methods","url":"https://www.semanticscholar.org/paper/6bc7aa121a15bae1ca300e745b39a7392a510cc0","abstract":"Continuous data are often measured or used in binned or rounded form. In this paper we follow up on Hall's work analyzing the effect of using equally-spaced binned data in a kernel density estimator. It is shown that a surprisingly large amount of binning does not adversely affect the integrated mean squared error of a kernel estimate.","tldr":"It is shown that a surprisingly large amount of binning does not adversely affect the integrated mean squared error of a kernel estimate."},{"id":"Scott:1992","doi":"10.1002/9780470316849","s2id":"6908b1db05f73a6d5d7fe70b8c577d8bf6df3c91","year":1992,"author":[{"given":"David W","family":"Scott"}],"title":"Multivariate Density Estimation: Theory, Practice, and Visualization","venue":"Wiley Series in Probability and Statistics","url":"https://www.semanticscholar.org/paper/6908b1db05f73a6d5d7fe70b8c577d8bf6df3c91"},{"id":"Sheather:1991","doi":"10.1111/j.2517-6161.1991.tb01857.x","s2id":"738f063d62eb02e41872679ea76e5c5a1553ff92","year":1991,"author":[{"given":"S. J.","family":"Sheather"},{"given":"M. C.","family":"Jones"}],"title":"A reliable data-based bandwidth selection method for kernel density estimation","venue":"Journal of the Royal Statistical Society, Series B (Methodological)","url":"https://www.semanticscholar.org/paper/738f063d62eb02e41872679ea76e5c5a1553ff92","abstract":"We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non- stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.","tldr":"The key to the success of the current procedure is the reintroduction of a non- stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance."},{"id":"Silverman:1982","doi":"10.2307/2347084","s2id":"ac387b2a1837a5aa294021900b664c4feeeb9cf9","year":1982,"author":[{"given":"B. W.","family":"Silverman"}],"title":"Algorithm AS 176: Kernel density estimation using the fast Fourier transform","venue":"Journal of the Royal Statistical Society, Series C (Applied Statistics)","url":"https://www.semanticscholar.org/paper/ac387b2a1837a5aa294021900b664c4feeeb9cf9"},{"id":"MASS:2002","doi":"10.1007/978-0-387-21706-2","s2id":"e31606c0cdb2b2cf1a8c749dd71402053b8f2b12","year":2010,"author":[{"given":"W. N.","family":"Venables"},{"given":"B. D.","family":"Ripley"}],"title":"Modern applied statistics with S","url":"https://www.semanticscholar.org/paper/e31606c0cdb2b2cf1a8c749dd71402053b8f2b12","tldr":"A guide to using S environments to perform statistical analyses providing both an introduction to the use of S and a course in modern statistical methods."},{"id":"Wand:1994","doi":"10.2307/1390904","year":1994,"author":[{"given":"M. P.","family":"Wand"}],"title":"Fast Computation of Multivariate Kernel Estimators","venue":"Journal of Statistical Computation and Simulation"},{"id":"Wells:1986","doi":"10.1109/TPAMI.1986.4767776","s2id":"305b55e2fef5679878313933c1bf4ee0251ce53c","year":1986,"author":[{"given":"W. M.","family":"Wells"}],"title":"Efficient synthesis of Gaussian filters by cascaded uniform filters","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","url":"https://www.semanticscholar.org/paper/305b55e2fef5679878313933c1bf4ee0251ce53c","abstract":"Gaussian filtering is an important tool in image processing and computer vision. In this paper we discuss the background of Gaussian filtering and look at some methods for implementing it. Consideration of the central limit theorem suggests using a cascade of ``simple'' filters as a means of computing Gaussian filters. Among ``simple'' filters, uniform-coefficient finite-impulse-response digital filters are especially economical to implement. The idea of cascaded uniform filters has been around for a while [13], [16]. We show that this method is economical to implement, has good filtering characteristics, and is appropriate for hardware implementation. We point out an equivalence to one of Burt's methods [1], [3] under certain circumstances. As an extension, we describe an approach to implementing a Gaussian Pyramid which requires approximately two addition operations per pixel, per level, per dimension. We examine tradeoffs in choosing an algorithm for Gaussian filtering, and finally discuss an implementation.","tldr":"This paper describes an approach to implementing a Gaussian Pyramid which requires approximately two addition operations per pixel, per level, per dimension, and examines tradeoffs in choosing an algorithm for Gaussian filtering."},{"id":"Wickham:2009","doi":"10.1007/978-0-387-98141-3","year":2009,"author":[{"given":"Hadley","family":"Wickham"}],"title":"ggplot2: Elegant Graphics for Data Analysis"}]}}}